#!/bin/bash
#SBATCH --partition=gpu
#SBATCH --gpus=1
#SBATCH --cpus-per-task=18
#SBATCH --time=00:25:00
#SBATCH --mem=12000M
#SBATCH --job-name=rs_dry
#SBATCH --array=1-9   # Adjust the range (1-n) to the number of n instances you want to run
#SBATCH --output=logs/output_rs_dry_%A_%a.log
#SBATCH --error=logs/error_rs_dry_%A_%a.log

# Load any required modules or environments (if needed)
module purge
module load 2021
module load Anaconda3/2021.05

# Your job starts in the directory where you call sbatch
# Activate your environment

source activate LC

# Change to the working directory (where your Python script is located)
cd $HOME/Projects/LCOpt

algos=("RandomSearch")
segments=(2 3 4)
repeats=(10)
iters=(10000)
samples=("sample_mix_3dist")
wet=(False)
# note that peak_purity only uses dry, and is super expensive.
# note that everything using Kaiser uses wet, these are sum_of_kais, prod_of_kais, tyteca28, tyteca35, tyteca40
crfs=('sum_of_res' 'prod_of_res' 'crf')

# how many combinations of parameters are there?
# 3 * 3 = 9

# Calculate the indices for each option based on the task ID
task_id=$((SLURM_ARRAY_TASK_ID - 1))
algo_index=$((task_id % 1))
task_id=$((task_id / 1))
segment_index=$((task_id % 3))
task_id=$((task_id / 3))
repeat_index=$((task_id % 1))
task_id=$((task_id / 1))
iter_index=$((task_id % 1))
task_id=$((task_id / 1))
sample_index=$((task_id % 1))
task_id=$((task_id / 1))
wet_index=$((task_id % 1))
task_id=$((task_id / 1))
crf_index=$((task_id % 3))

# Retrieve the specific options for each parameter based on the calculated indices
algo=${algos[algo_index]}
segment=${segments[segment_index]}
repeat=${repeats[repeat_index]}
iter=${iters[iter_index]}
sample=${samples[sample_index]}
wet_value=${wet[wet_index]}
crf=${crfs[crf_index]}

# Execute your Python script with the selected options
srun python meta_experiment.py "$algo" "$segment" "$repeat" "$iter" "$sample" "$wet_value" "$crf"
